{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CreateDatasets\n",
    "=====\n",
    "***\n",
    "\n",
    "## Importing modules\n",
    "This jupyter notebook creates labelled data from the raw acceleration magnitude streams. It does not explain the format of the UniMiB-SHAR dataset that was primarily used (for that, see InvestigateDataset.ipynb).\n",
    "\n",
    "The first step is to import the modules needed for calculation and data processing.\n",
    "* `numpy` is necessary for various statistic calculations used in obtaining features from signal windows such as the mean (average), standard deviation, IQR (inter-quartile range), calculating the sum of signals (discrete integral), and performing Fourier Transforms when calculating power and energy features.\n",
    "* `scipy.signal` is necessary for estimating auto-spectral density of signals (`welch`) and coherence (`coherence`) of features\n",
    "* `scipy.io` is used for loading the UniMiB-SHAR dataset from a MatLab (`.mat`) file into numpy arrays\n",
    "* `math.modf` is a function that splits a floating point number into its whole and decimal parts. It is used for splitting the signals into windows. `math.sqrt` takes the square root of a number and is used for `nperseg` calculations.\n",
    "* `itertools.combinations` is a function that returns all possible combinations (nCr) from a list of length *n* and specified *r*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import welch, coherence\n",
    "import scipy.io as sio\n",
    "from math import modf, sqrt\n",
    "from itertools import combinations\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Next is deciding what features we wish to extract from each window of the accelerometer streams. We chose the same features that were used in [similar research](https://www.sciencedirect.com/science/article/pii/S1574119212000703 \"Recognizing whether sensors are on the same body\"): mean, standard deviation, variance, mean absolute deviation, IQR, energy, and power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(sig):\n",
    "    '''Returns the mean value of a signal'''\n",
    "    return np.mean(sig)\n",
    "\n",
    "def get_std(sig):\n",
    "    '''Returns the standard deviation of a signal'''\n",
    "    return np.std(sig)\n",
    "\n",
    "def get_variance(sig):\n",
    "    '''Returns the variance of a signal'''\n",
    "    return get_std(sig)**2\n",
    "\n",
    "def get_mad(sig):\n",
    "    '''Returns the mean absolute deviation of a signal'''\n",
    "    m = get_mean(sig)\n",
    "    sig = [ abs(x - m) for x in sig ]\n",
    "    return get_mean(sig)\n",
    "\n",
    "def get_iqr(sig):\n",
    "    '''Returns the IQR of a signal'''\n",
    "    q75, q25 = np.percentile(sig, [75 ,25])\n",
    "    return q75 - q25\n",
    "\n",
    "def get_energy(sig, dt):\n",
    "    '''\n",
    "    Returns the energy of a signal, approximated using scipy.signal.welch\n",
    "        sig - the signal\n",
    "        dt - the time difference between successive samples in sig\n",
    "    '''\n",
    "    f_welch, S_xx_welch = welch(sig, fs=1/dt, nperseg=int(sqrt(len(sig))))\n",
    "    df_welch = f_welch[1] - f_welch[0]\n",
    "    f_fft = np.fft.fftfreq(len(sig), d=dt)\n",
    "    df_fft = f_fft[1] - f_fft[0]\n",
    "    E_welch = (1. / dt) * (df_welch / df_fft) * np.sum(S_xx_welch)\n",
    "    return E_welch\n",
    "\n",
    "def get_power(sig, dt):\n",
    "    '''\n",
    "    Returns the power of a signal, approximated using scipy.signal.welch\n",
    "        sig - the signal\n",
    "        dt - the time difference between successive samples in sig\n",
    "    '''\n",
    "    f_welch, S_xx_welch = welch(sig, fs=1/dt, nperseg=int(sqrt(len(sig))))\n",
    "    df_welch = f_welch[1] - f_welch[0]\n",
    "    P_welch = np.sum(S_xx_welch) * df_welch\n",
    "    return P_welch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we write a function to return a vector of these features, given a window (section of an accelerometer stream). In `get_feature_matrix`, the signal is split into windows of equal length *w*. Then each of these windows is passed to `get_feature_vector` which returns the corresponding feature vector. That vector is then added to the feature matrix, which is returned from `get_feature_matrix` after all windows have been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(window, dt):\n",
    "    '''\n",
    "    Returns the feature vector of a window\n",
    "        window - the window used to calculate the feature vector\n",
    "        dt - the time difference between successive samples in sig\n",
    "    Features are: mean, std, variance, mad, iqr, energy, and power\n",
    "    '''\n",
    "    mean     = get_mean(window)\n",
    "    std      = get_std(window)\n",
    "    variance = get_variance(window)\n",
    "    mad      = get_mad(window)\n",
    "    iqr      = get_iqr(window)\n",
    "    energy   = get_energy(window, dt)\n",
    "    power    = get_power(window, dt)\n",
    "    return np.array([mean, std, variance, mad, iqr, energy, power])\n",
    "\n",
    "def get_feature_matrix(sig, w, dt):\n",
    "    '''\n",
    "    Returns the feature matrix of a signal\n",
    "        sig - the signal used\n",
    "        w - the length of each window for extracting feature vectors\n",
    "        dt - the time difference between successive samples in sig\n",
    "    '''\n",
    "    windows = split(sig, w)\n",
    "        \n",
    "    matrix = []\n",
    "    for window in windows:\n",
    "        f = get_feature_vector(window, dt)\n",
    "        matrix.append(f)\n",
    "    \n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last a few helper functions to split a signal into windows and get the *c* value used when calculating coherence..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(sig, w):\n",
    "    '''\n",
    "    Splits a signal into windows. Returns an array of smaller signals of length w\n",
    "        sig - the signal to split\n",
    "        w - the window length in samples\n",
    "    '''\n",
    "    num_windows = float(len(sig)) / w\n",
    "    dec, i = modf(num_windows)\n",
    "    if num_windows != int(num_windows):\n",
    "        cutoff = dec * w\n",
    "        last = int(round(-1*cutoff))\n",
    "        sig = sig[:last]\n",
    "        num_windows = i\n",
    "    return np.split(sig, num_windows)\n",
    "\n",
    "def get_c(coherence_window, w, dt):\n",
    "    '''\n",
    "    Converts coherence window (seconds) into c (windows). Used when determining number of samples at a time \n",
    "    for coherence calculation.\n",
    "    '''\n",
    "    return int((coherence_window) / (w * dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence\n",
    "\n",
    "Next we write functions for calculating the normalized coherence of features across a window *c*, and then the normalized coherence of the whole matrix. Each cell of the normalized coherence matrix corresponds to calculating the normalized coherence starting in the same row/column, and including features from *c* rows down. This normalized coherence matrix essentially represents how well two signals are correlated with respect to each feature.\n",
    "\n",
    "\n",
    "# ( !!!!! TODO : ADD A VISUALIZATION OF ^ !!!!! )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_coherence(f1, f2, phi_max, dt):\n",
    "    '''\n",
    "    Calculate the normalized coherence of two feature observances f1 and f2 with a specified phi_max\n",
    "        f1 - one of the feature observances\n",
    "        f2 - the other feature observance\n",
    "        phi_max - the maximum frequency to be considered\n",
    "        dt - the time difference between successive samples in sig\n",
    "    '''\n",
    "    f, C_xy = coherence(f1, f2, 1/(dt * w), nperseg=int(sqrt( (len(f1) + len(f2)) / 2 )) )\n",
    "    f[f < phi_max]\n",
    "    C_xy = C_xy[:len(f)]\n",
    "    return 1/float(phi_max) * np.sum(C_xy)\n",
    "\n",
    "def matrix_coherence(A, B, c, phi_max, dt):\n",
    "    '''\n",
    "    Calculate the normalized coherence matrix of two feature matrices A and B\n",
    "        A - a feature matrix\n",
    "        B - a feature matrix\n",
    "        c - the number of windows considered at a time when calculating coherence of feature observances\n",
    "        phi_max - the maximum frequency considered\n",
    "        dt - the time difference between successive samples in sig\n",
    "    '''\n",
    "    num_windows = len(A)\n",
    "    rows = num_windows - (c - 1)\n",
    "    num_features = len(A[0])\n",
    "    matrix = np.empty([rows, num_features])\n",
    "\n",
    "    for f in range(0, 7):\n",
    "        A_feature = np.transpose(A)[f]\n",
    "        B_feature = np.transpose(B)[f]\n",
    "\n",
    "        for k in range(0, rows):\n",
    "            A_samples = A_feature[k:k+c]\n",
    "            B_samples = B_feature[k:k+c]\n",
    "            cell = feature_coherence(A_samples, B_samples, phi_max, dt)\n",
    "\n",
    "            matrix[k][f] = cell\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all functions necessary to compute what we're after, the normalized coherence matrix given two magnitude streams, we must write functions to generate the labelled data used in the machine learning algorithms.\n",
    "\n",
    "***\n",
    "\n",
    "## Reorganizing the raw data\n",
    "\n",
    "First, two functions `get_all_trials_UniMiB-SHAR`, and `get_all_trials_COLLECTED` to loop through the raw data (UniMiB-SHAR data and data we collected manually, respectively), and reorganize it into a list of every magnitude stream along with the person it came from. Then `get_pairs` that loops through the list of all trials and returns a list of tuples, each containing a pair of accelerometer streams; and the whole list containing every possible combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_trials_UniMiBSHAR(data, people, activities):\n",
    "    '''\n",
    "    Loops through the raw UniMiB-SHAR dataset and reorganizes all requested data into an easier to use list\n",
    "        data - the raw dataset consisting of many nested numpy arrays\n",
    "        activities - the requested activities\n",
    "    '''\n",
    "    all_trials = []\n",
    "    # Loop through dataset and append trials to new_data\n",
    "    for p in people:\n",
    "        accel_data = data[p][0][0][0]\n",
    "        \n",
    "        # Loop through trials and append magnitude streams to trial list\n",
    "        for a in activities:\n",
    "            activity = accel_data[a]\n",
    "            for t in range(len(activity)):\n",
    "                trial = activity[t][0]\n",
    "                magnitude = trial[5]\n",
    "                all_trials.append([p, magnitude])\n",
    "                \n",
    "    return all_trials\n",
    "\n",
    "def get_all_trials_OURS(data):\n",
    "    '''\n",
    "    Loops through our raw dataset and reorganizes all requested data into an easier to use list\n",
    "        data - the raw dataset consisting of many nested numpy arrays\n",
    "        activities - the requested activities\n",
    "    '''\n",
    "    all_trials = []\n",
    "        \n",
    "    for p, person in enumerate(data):\n",
    "        for trial in person:\n",
    "            for side in trial:\n",
    "                magnitude = side[1]\n",
    "                all_trials.append([p, np.array(magnitude)])\n",
    "                \n",
    "    # have to delete 15 and 22 because it is very short (sampling must have messed up)\n",
    "    del all_trials[15]\n",
    "    del all_trials[22]\n",
    "    \n",
    "    return all_trials\n",
    "\n",
    "def get_pairs(trials):\n",
    "    '''\n",
    "    Get a list of all possible combinations of magnitude streams from a list of all trials\n",
    "    '''\n",
    "    return list(combinations(trials, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "Once we have a list of all possible combinations of trials and the people they belong to, we can calculate a coherence matrix in `get_coherence_matrix` using the functions we defined above from each pair. This matrix is then labelled in `process` with a `1` if the two streams came from the same person, or a `0` if they came from different people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coherence_matrix(trial1, trial2, w, c, dt):\n",
    "    '''\n",
    "    Calculate a coherence matrix of two magnitude streams and label it\n",
    "        trial1 - The first trial of the format [person index, magnitude stream]\n",
    "        trial2 - The second trial of the format [person index, magnitude stream]\n",
    "        w - the window length used when extracting feature vectors\n",
    "        c - the coherence window used when calculating feature coherences\n",
    "        dt - the time difference between successive samples in sig\n",
    "    '''\n",
    "    person1 = trial1[0]\n",
    "    sig1 = trial1[1]\n",
    "    person2 = trial2[0]\n",
    "    sig2 = trial2[1]\n",
    "    \n",
    "    short = min(len(sig1), len(sig2))\n",
    "    sig1 = sig1[:short]\n",
    "    sig2 = sig2[:short]\n",
    "    \n",
    "    A = get_feature_matrix(sig1, w, dt)\n",
    "    B = get_feature_matrix(sig2, w, dt)\n",
    "    phi_max = 10\n",
    "    return matrix_coherence(A, B, c, phi_max, dt)\n",
    "\n",
    "def process(pairs, w, c, dt, verbose=False):\n",
    "    '''\n",
    "    Loops through all pairs of trials, calculates the coherence matrix, labels it appropriately, then splits\n",
    "    each by its rows and labels those with the corresponding label.\n",
    "        pairs - the list of all pairs of trials\n",
    "        w - the window length used when extracting feature vectors\n",
    "        c - the coherence window used when calculating feature coherences\n",
    "        dt - the time difference between successive samples in sig\n",
    "    '''\n",
    "    labelled_matrices = []\n",
    "    for pair in pairs:\n",
    "        if verbose >= 2:\n",
    "            print('{} {}'.format(str(pair[0][0]), str(pair[1][0])))\n",
    "        labelled_matrices.append([get_coherence_matrix(*pair, w, c, dt), (pair[0][0] == pair[1][0])])\n",
    "    \n",
    "    labelled_rows = []\n",
    "    for x in labelled_matrices:\n",
    "        new_row = []\n",
    "        matrix = x[0]\n",
    "        label = x[1]\n",
    "        for fc in matrix:\n",
    "            labelled_rows.append([list(fc), label])\n",
    "    \n",
    "    return labelled_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a fucntion to create a dataset given a list of people, a list of activities, window length, and coherence length. If person 18 is included, we remove them since the data recorded for them in the UniMiB-SHAR dataset were much shorter than the rest and thus couldn't be used effectively. To generate the labelled dataset we simply use `get_all_trials_UniMiBSHAR` and `get_pairs` in succession to get all possible pairs of accelerometer streams. Then we use `get_c` to convert a coherence window in seconds to windows and finally `process` to get the labelled data we have been after this whole time.\n",
    "\n",
    "(Because of how the data is processed, some combinations of *w* and *cw* don't work together. When this happens, we want to skip the generating the file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_UniMiBSHAR(full_data, people, activities, w, cw, dt, verbose=1):\n",
    "    '''\n",
    "    Creates a labelled dataset from the UniMiB-SHAR raw data given people, activities, w, cw, and dt\n",
    "        people - a list of people to be included in the dataset\n",
    "        activities - a list of activities to be included in the dataset\n",
    "        w - the window length used when extracting feature vectors\n",
    "        c - the coherence window used when calculating feature coherences\n",
    "        dt - the time difference between successive samples in sig\n",
    "    '''\n",
    "    if 19 in people:\n",
    "        people.remove(19) # delete person index 19 because short stream\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print('creating dataset with\\n  people {}\\n  activities {}\\n  w={}\\n  cw={}'.format(people, activities, w, cw))\n",
    "    \n",
    "    try:\n",
    "        trials = gete_all_trials_UniMiBSHAR(full_data)\n",
    "        c = get_c(cw, w, dt)\n",
    "        labelled = process(pairs, w, c, dt, verbose)\n",
    "        return labelled\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Invalid combination of w and cw, file skipped')\n",
    "        print('(error: {})'.format(e))\n",
    "        return\n",
    "\n",
    "    \n",
    "    \n",
    "def create_dataset_OURS(data, w, cw, dt, verbose=1):\n",
    "    '''\n",
    "    Creates a labelled dataset given from our manually collected raw data\n",
    "        people - a list of people to be included in the dataset\n",
    "        activities - a list of activities to be included in the dataset\n",
    "        w - the window length used when extracting feature vectors\n",
    "        c - the coherence window used when calculating feature coherences\n",
    "        dt - the time difference between successive samples in sig\n",
    "    '''\n",
    "    trials = get_all_trials_OURS(data)\n",
    "    pairs = get_pairs(trials)\n",
    "    c = get_c(cw, w, dt)\n",
    "    labelled = process(pairs, w, c, dt, verbose)\n",
    "    return labelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally have everything that we need to generate a dataset. First we load the raw `.mat` file into a variable using `scipy.io` and get the actual data into the variable `full_data`. Next we define `dt` depending on the sampling rate of the dataset being used (UniMiB-SHAR is 50 Hz). Last but not least, we call the `create_dataset` function and store what it returns in a variable. This can now be used with classification machine learning algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "# mat = sio.loadmat('./data/UniMiB-SHAR/data/full_data.mat') # UniMiB-SHAR dataset\n",
    "# full_data = mat['full_data']\n",
    "\n",
    "data = np.load('./data/collected_clean/magnitude_data_clean.npy')\n",
    "\n",
    "dt = 0.02\n",
    "\n",
    "# people = list(range(30))\n",
    "# activities = [2]\n",
    "w = 4\n",
    "cw = 15\n",
    "\n",
    "# dataset = create_dataset_UniMiBSHAR(data, people, activities, w, cw, dt, verbose=2)\n",
    "dataset = create_dataset_OURS(data, w, cw, dt, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset was successfully created, we can save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'p' + str(len(people)) + '_a' + str(activities) + '_w' + str(w) + '_cw' + str(cw) + '.npy'\n",
    "# np.save('./data/created_UniMiB-SHAR/nperseg=sqrt/' + filename, dataset)\n",
    "\n",
    "filename = 'w' + str(w) + '_cw' + str(cw) + '.npy'\n",
    "np.save('./data/created_collected/' + filename, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since creating a dataset can take a while, a function that can create multiple datasets automatically would be very useful (for example running it overnight). To do so we will write a function that loops through all possible sets of people, activities, window lengths, and coherence windows specified and create and save a dataset for each with a descriptive name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(people_sets, activities, window_lengths, coherence_windows, dt, verbose=1):\n",
    "    '''\n",
    "    Creates multiple datasets and saves them all to a file\n",
    "        people_sets - a list of lists of people from which to create datasets\n",
    "        activities - a list of activities to be included in the dataset\n",
    "        window_lengths - window lengths to be used\n",
    "        coherence_windows - coherence windows to be used\n",
    "        dt - the time difference between successive samples in sig\n",
    "    '''\n",
    "    for people in people_sets:\n",
    "        for w in window_lengths:\n",
    "            for cw in coherence_windows:\n",
    "                filename = 'p' + str(len(people)) + '_a' + str(activities) + '_w' + str(w) + '_cw' + str(cw) + '.npy'\n",
    "                if verbose >= 1:\n",
    "                    print('starting {}'.format(filename))\n",
    "                try:\n",
    "                    dataset = create_dataset(people, activities, w, cw, dt, verbose)\n",
    "                    np.save('./data/nperseg=sqrt' + filename, dataset)\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                finally:\n",
    "                    print('finished {}'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally move on to the fun part: machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
