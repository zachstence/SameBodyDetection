{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import scipy.io as sio\n",
    "from math import modf\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a global variable for the sample rate of the UniMiB-SHAR dataset, and how many features we are using (to be changed later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = 0.02\n",
    "FS = 1/DT\n",
    "NUM_FEATURES = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to create functions to extract features from an accelerometer stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(sig):\n",
    "    return np.mean(sig)\n",
    "\n",
    "def get_std(sig):\n",
    "    return np.std(sig)\n",
    "\n",
    "def get_variance(sig):\n",
    "    return get_std(sig)**2\n",
    "\n",
    "def get_mad(sig):\n",
    "    m = get_mean(sig)\n",
    "    sig = [ abs(x - m) for x in sig ]\n",
    "    return get_mean(sig)\n",
    "\n",
    "def get_iqr(sig):\n",
    "    q75, q25 = np.percentile(sig, [75 ,25])\n",
    "    return q75 - q25\n",
    "\n",
    "def get_energy(sig):\n",
    "    # Not sure what nperseg should be here\n",
    "    f_welch, S_xx_welch = signal.welch(sig, fs=FS, nperseg=len(sig)/2)\n",
    "    df_welch = f_welch[1] - f_welch[0]\n",
    "    dt = 1/FS\n",
    "    f_fft = np.fft.fftfreq(len(sig), d=dt)\n",
    "    df_fft = f_fft[1] - f_fft[0]\n",
    "    E_welch = (1. / dt) * (df_welch / df_fft) * np.sum(S_xx_welch)\n",
    "    return E_welch\n",
    "\n",
    "def get_power(sig):\n",
    "    # Not sure what nperseg should be here\n",
    "    f_welch, S_xx_welch = signal.welch(sig, fs=FS, nperseg=len(sig)/2)\n",
    "    df_welch = f_welch[1] - f_welch[0]\n",
    "    P_welch = np.sum(S_xx_welch) * df_welch\n",
    "    return P_welch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we write a function to return a vector of these features, given a window (section of an accelerometer stream),\n",
    "\n",
    "And a function to create a matrix of these feature vectors for an accelerometer stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(sig):\n",
    "    mean     = get_mean(sig)\n",
    "    std      = get_std(sig)\n",
    "    variance = get_variance(sig)\n",
    "    mad      = get_mad(sig)\n",
    "    iqr      = get_iqr(sig)\n",
    "    energy   = get_energy(sig)\n",
    "    power    = get_power(sig)\n",
    "    return np.array([mean, std, variance, mad, iqr, energy, power])\n",
    "\n",
    "def get_feature_matrix(sig, w):\n",
    "    num_windows = int(len(sig) / w)\n",
    "    windows = split(sig, w)\n",
    "\n",
    "    matrix = np.empty([num_windows, NUM_FEATURES])\n",
    "\n",
    "    index = 0\n",
    "    for window in windows:\n",
    "        f = get_feature_vector(window)\n",
    "        matrix[index] = f\n",
    "        index += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag(sig):\n",
    "    return np.linalg.norm(sig)\n",
    "\n",
    "def split(sig, w):\n",
    "    num_windows = float(len(sig)) / w\n",
    "    dec, i = modf(num_windows)\n",
    "    if num_windows != int(num_windows):\n",
    "        cutoff = dec * w\n",
    "        last = int(round(-1*cutoff))\n",
    "        sig = sig[:last]\n",
    "        num_windows = i\n",
    "    return np.split(sig, num_windows)\n",
    "\n",
    "def get_c(w, coherence_window):\n",
    "    c = int((coherence_window) / (w * DT))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the signal processing functions to get the coherence of two vectors, the normalized coherence of two signals, and the normalized coherence matrix of two feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence(sig1, sig2):\n",
    "    coherence = signal.coherence(sig1, sig2\n",
    "        , FS    # I think this may be wrong because when we compute coherence its between two coherence windows containing samples, so should it be DT or window length? Also not sure how it affects the math at all\n",
    "        , nperseg=len(sig1)/2) # Not sure what nperseg should be\n",
    "    return coherence\n",
    "\n",
    "def N_signal(sig1, sig2, phi_max):\n",
    "    f, C_xy = coherence(sig1, sig2)\n",
    "    f[f < 10]\n",
    "    C_xy = C_xy[:len(f)]\n",
    "    return 1/float(phi_max) * np.sum(C_xy)\n",
    "\n",
    "def N_matrix(A, B, c, phi_max):\n",
    "    num_windows = len(A)\n",
    "    rows = num_windows - (c - 1)\n",
    "    matrix = np.empty([rows, NUM_FEATURES])\n",
    "\n",
    "    for f in range(0, 7):\n",
    "        A_feature = np.transpose(A)[f]\n",
    "        B_feature = np.transpose(B)[f]\n",
    "\n",
    "        for k in range(0, rows):\n",
    "            A_samples = A_feature[k:k+c]\n",
    "            B_samples = B_feature[k:k+c]\n",
    "            cell = N_signal(A_samples, B_samples, phi_max)\n",
    "\n",
    "            matrix[k][f] = cell\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write code to go through the dataset and choose every combination of two accelerometer streams\n",
    "\n",
    "(REALLY NEED TO ADD VISUALIZATIONS HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a function to split the each person's trials into separate arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_trials(data):\n",
    "    all_trials = []\n",
    "    \n",
    "    # append each magnitude stream to all_trials\n",
    "    for p in range(len(data)):\n",
    "      for t in range(len(data[p])):\n",
    "        mag = data[p][t]\n",
    "        all_trials.append([p, mag])\n",
    "    \n",
    "    return all_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a function that returns every possible pair of trials from an array of all the trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(trials):\n",
    "    return list(combinations(trials, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a function to get the coherence matrix given two trials and a function to loop through all the pairs and get the coherence matrix and label it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coherence_matrix(w, c, trial1, trial2):\n",
    "    person1 = trial1[0]\n",
    "    sig1 = trial1[1]\n",
    "    person2 = trial2[0]\n",
    "    sig2 = trial2[1]\n",
    "    \n",
    "    short = min(len(sig1), len(sig2))\n",
    "    sig1 = sig1[:short]\n",
    "    sig2 = sig2[:short]\n",
    "    \n",
    "    A = get_feature_matrix(sig1, w)\n",
    "    B = get_feature_matrix(sig2, w)\n",
    "    phi_max = 10\n",
    "    return N_matrix(A, B, c, phi_max)\n",
    "\n",
    "def process(pairs, w, c):\n",
    "    labelled_data = []\n",
    "    for pair in pairs:\n",
    "        print(str(pair[0][0]) + \" \" + str(pair[1][0]))\n",
    "        labelled_data.append([get_coherence_matrix(w, c, *pair), (pair[0][0] == pair[1][0])])\n",
    "    return labelled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we need to split each labelled coherence matrix by its rows and label them as well, in this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_rows(labelled):\n",
    "    all_rows = []\n",
    "    for row in labelled:\n",
    "        new_row = []\n",
    "        matrix = row[0]\n",
    "        label = row[1]\n",
    "        for fc in matrix:\n",
    "            all_rows.append([list(fc), label])\n",
    "    return all_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can finally generate labelled data that we can use with skearn classifiers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so we first load the raw dataset. Then we specify parameters for the dataset generation (number of people, window length, and coherence window length) then through several loops we generate and save each datafile with a name describing the parameters used to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p10_w4_cw9.npy\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 2\n",
      "0 3\n",
      "0 3\n",
      "0 4\n",
      "0 4\n",
      "0 5\n",
      "0 5\n",
      "0 6\n",
      "0 6\n",
      "0 7\n",
      "0 7\n",
      "0 8\n",
      "0 8\n",
      "0 9\n",
      "0 9\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 2\n",
      "0 3\n",
      "0 3\n",
      "0 4\n",
      "0 4\n",
      "0 5\n",
      "0 5\n",
      "0 6\n",
      "0 6\n",
      "0 7\n",
      "0 7\n",
      "0 8\n",
      "0 8\n",
      "0 9\n",
      "0 9\n",
      "1 1\n",
      "1 2\n",
      "1 2\n",
      "1 3\n",
      "1 3\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 6\n",
      "1 6\n",
      "1 7\n",
      "1 7\n",
      "1 8\n",
      "1 8\n",
      "1 9\n",
      "1 9\n",
      "1 2\n",
      "1 2\n",
      "1 3\n",
      "1 3\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 6\n",
      "1 6\n",
      "1 7\n",
      "1 7\n",
      "1 8\n",
      "1 8\n",
      "1 9\n",
      "1 9\n",
      "2 2\n",
      "2 3\n",
      "2 3\n",
      "2 4\n",
      "2 4\n",
      "2 5\n",
      "2 5\n",
      "2 6\n",
      "2 6\n",
      "2 7\n",
      "2 7\n",
      "2 8\n",
      "2 8\n",
      "2 9\n",
      "2 9\n",
      "2 3\n",
      "2 3\n",
      "2 4\n",
      "2 4\n",
      "2 5\n",
      "2 5\n",
      "2 6\n",
      "2 6\n",
      "2 7\n",
      "2 7\n",
      "2 8\n",
      "2 8\n",
      "2 9\n",
      "2 9\n",
      "3 3\n",
      "3 4\n",
      "3 4\n",
      "3 5\n",
      "3 5\n",
      "3 6\n",
      "3 6\n",
      "3 7\n",
      "3 7\n",
      "3 8\n",
      "3 8\n",
      "3 9\n",
      "3 9\n",
      "3 4\n",
      "3 4\n",
      "3 5\n",
      "3 5\n",
      "3 6\n",
      "3 6\n",
      "3 7\n",
      "3 7\n",
      "3 8\n",
      "3 8\n",
      "3 9\n",
      "3 9\n",
      "4 4\n",
      "4 5\n",
      "4 5\n",
      "4 6\n",
      "4 6\n",
      "4 7\n",
      "4 7\n",
      "4 8\n",
      "4 8\n",
      "4 9\n",
      "4 9\n",
      "4 5\n",
      "4 5\n",
      "4 6\n",
      "4 6\n",
      "4 7\n",
      "4 7\n",
      "4 8\n",
      "4 8\n",
      "4 9\n",
      "4 9\n",
      "5 5\n",
      "5 6\n",
      "5 6\n",
      "5 7\n",
      "5 7\n",
      "5 8\n",
      "5 8\n",
      "5 9\n",
      "5 9\n",
      "5 6\n",
      "5 6\n",
      "5 7\n",
      "5 7\n",
      "5 8\n",
      "5 8\n",
      "5 9\n",
      "5 9\n",
      "6 6\n",
      "6 7\n",
      "6 7\n",
      "6 8\n",
      "6 8\n",
      "6 9\n",
      "6 9\n",
      "6 7\n",
      "6 7\n",
      "6 8\n",
      "6 8\n",
      "6 9\n",
      "6 9\n",
      "7 7\n",
      "7 8\n",
      "7 8\n",
      "7 9\n",
      "7 9\n",
      "7 8\n",
      "7 8\n",
      "7 9\n",
      "7 9\n",
      "8 8\n",
      "8 9\n",
      "8 9\n",
      "8 9\n",
      "8 9\n",
      "9 9\n",
      "- p10_w4_cw9.npy finished\n"
     ]
    }
   ],
   "source": [
    "mat = sio.loadmat('./UniMiB-SHAR/data/full_data.mat')\n",
    "full_data = mat['full_data']\n",
    "\n",
    "people = [10]\n",
    "windows = [4]\n",
    "coherence_windows = [9]\n",
    "\n",
    "for p in people:\n",
    "    data = full_data[:p]\n",
    "    if p > 18:\n",
    "        data = np.delete(data, (19), axis=0) # delete person 19 because short stream\n",
    "    \n",
    "    new_data = []\n",
    "    # Loop through dataset and append trials to new_data\n",
    "    for person_index in range(len(data)):\n",
    "        accel_data = data[person_index][0][0][0]\n",
    "        activity_index = 2 # walking\n",
    "        activity = accel_data[activity_index]\n",
    "        \n",
    "        # Loop through trials and append magnitude streams to trial list\n",
    "        trials = []\n",
    "        for t in range(len(activity)):\n",
    "          trial = activity[t][0]\n",
    "          magnitude = trial[5]\n",
    "          trials.append(magnitude)\n",
    "          \n",
    "        new_data.append(trials)\n",
    "    \n",
    "    trials = get_all_trials(new_data)\n",
    "    pairs = get_pairs(trials)\n",
    "    \n",
    "    for w in windows:\n",
    "        for cw in coherence_windows:\n",
    "            filename = 'p' + str(p) + '_w' + str(w) + '_cw' + str(cw) + '.npy'\n",
    "            try:\n",
    "                print(filename)\n",
    "                \n",
    "                c = get_c(w, cw)\n",
    "                \n",
    "                labelled = process(pairs, w, c)\n",
    "                \n",
    "                rows = get_all_rows(labelled)\n",
    "                \n",
    "                np.save('./data/' + filename, rows)\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('- ' + filename + ' skipped')\n",
    "                continue\n",
    "            finally:\n",
    "                print('- ' + filename + ' finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
