{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune\n",
    "=====\n",
    "***\n",
    "\n",
    "## Importing modules\n",
    "This notebook trains and tests a model and dataset of your choice with various parameters using scikit-learn's `RandomizedSearchCV` and `GridSearchCV` functions to attempt to optimize the hyperparameters of the model. It stores the results into a separate file.\n",
    "\n",
    "The first step is to import the modules needed for calculation and data processing.\n",
    "* `numpy` is necessary for loading the dataset chosen\n",
    "* `sklearn.model_selection.GridSearchCV` and `RandomizedSearchCV` are the functions from scikit-learn that test a classifier with various parameters and returns the set of best parameters.\n",
    "* `time` is used for timing the train and test time for a classifier\n",
    "* `sklearn.metrics.accuracy_score`, `precision_score`, and `f1_score` are used to evalutate how well the optimized classifier performs\n",
    "* `scipy` is used to generate some ranges for the parameters in some classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide which dataset to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[list([0.043889873807057334, 0.02790274654327562, 0.056384859069108645, 0.028407378540714956, 0.029909518877672837, 0.06268918870596274, 0.06268918870596274])\n",
      "  True]\n",
      " [list([0.038555549796496724, 0.034047958172445165, 0.07140860072408488, 0.030144429004604824, 0.023216501502549575, 0.048193072266976236, 0.048193072266976236])\n",
      "  True]\n",
      " [list([0.016550405734261166, 0.0412423178052609, 0.07633630255419359, 0.037685888869649437, 0.03076987826785081, 0.07096448790532905, 0.07096448790532905])\n",
      "  True]\n",
      " ...\n",
      " [list([0.21525735417382894, 0.07533349550830877, 0.03617445176176007, 0.07787616616811044, 0.07281928942714669, 0.0384134186245445, 0.0384134186245445])\n",
      "  True]\n",
      " [list([0.22803877724256935, 0.07675640546227899, 0.0458651909639872, 0.07879921538939309, 0.06205415748936275, 0.050775977163007815, 0.050775977163007815])\n",
      "  True]\n",
      " [list([0.219261816034031, 0.0898163263768038, 0.041367862295759075, 0.08885118735403907, 0.07084578181871606, 0.05663263087053835, 0.05663263087053835])\n",
      "  True]]\n",
      "datapoints: 42641\n",
      "num true datapoints: 986\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs.full.TrainTest as TrainTest\n",
    "\n",
    "p = 29\n",
    "a = [2]\n",
    "w = 4\n",
    "cw = 9\n",
    "\n",
    "path = './data/created_UniMiB-SHAR/nperseg=sqrt/'\n",
    "file = 'p' + str(p) + '_a' + str(a) + '_w' + str(w) + '_cw' + str(cw)\n",
    "\n",
    "# path = './data/created_collected/'\n",
    "# file = 'w' + str(w) + '_cw' + str(cw)\n",
    "ext = '.npy'\n",
    "\n",
    "data = np.load(path + file + ext)\n",
    "print(data)\n",
    "\n",
    "print('datapoints: {}'.format(len(data)))\n",
    "true_count = 0\n",
    "for d in data:\n",
    "    if d[1]:\n",
    "        true_count += 1\n",
    "print('num true datapoints: {}'.format(true_count))\n",
    "\n",
    "x_train, y_train, x_test, y_test = TrainTest.get_train_test(data, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for getting the best parameters for a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParams(clf, param_dict, search='random', n_iter=100, cv=3):\n",
    "    if search == 'random':\n",
    "        clf_search = RandomizedSearchCV(\n",
    "            estimator = clf, \n",
    "            param_distributions = param_dict, \n",
    "            n_iter = n_iter, \n",
    "            cv = cv,\n",
    "            verbose = 1,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "    elif search == 'grid':\n",
    "        clf_search = GridSearchCV(\n",
    "            estimator = clf,\n",
    "            param_grid = param_dict,\n",
    "            cv = cv,\n",
    "            verbose = 1,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "    clf_search.fit(x_train, y_train)\n",
    "    return clf_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to record the results of classifier with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_results(clf, clf_name, trials=10):\n",
    "    print('testing {} through {} trials'.format(clf_name, trials))\n",
    "    avg_acc = 0\n",
    "    avg_prec = 0\n",
    "    avg_f1 = 0\n",
    "    avg_train_time = 0\n",
    "    avg_test_time = 0\n",
    "    \n",
    "    for i in range(trials):\n",
    "        train_start = time.clock()\n",
    "        clf.fit(x_train, y_train)\n",
    "        train_end = time.clock()\n",
    "        \n",
    "        test_start = time.clock()\n",
    "        y_pred = clf.predict(x_test)\n",
    "        test_end = time.clock()\n",
    "        \n",
    "        avg_acc += accuracy_score(y_test, y_pred)\n",
    "        avg_prec += precision_score(y_test, y_pred)\n",
    "        avg_f1 += f1_score(y_test, y_pred)\n",
    "        avg_train_time += (train_end - train_start)\n",
    "        avg_test_time += (test_end - test_start)\n",
    "        print('trial {} / {} finished\\n'.format(i + 1, trials), end='\\r')\n",
    "    \n",
    "    avg_acc /= trials\n",
    "    avg_prec /= trials\n",
    "    avg_f1 /= trials\n",
    "    avg_train_time /= trials\n",
    "    avg_test_time /= trials\n",
    "    \n",
    "    with open('./results/collected/' + clf_name + '/' + file + '.txt', 'a+') as f:\n",
    "        f.write('(best = {})\\n'.format(best))\n",
    "        f.write('trials         : {}\\n'.format(trials))\n",
    "        f.write('avg acc        : {}\\n'.format(avg_acc))\n",
    "        f.write('avg prec       : {}\\n'.format(avg_prec))\n",
    "        f.write('avg f1         : {}\\n'.format(avg_f1))\n",
    "        f.write('avg_train_time : {}\\n'.format(avg_train_time))\n",
    "        f.write('avg_test_time  : {}\\n'.format(avg_test_time))\n",
    "        f.write('-----\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define classifier and parameters to search through, then call functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 800 candidates, totalling 8000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1024 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2524 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4624 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 6037 tasks      | elapsed:   56.6s\n",
      "[Parallel(n_jobs=-1)]: Done 7687 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7985 out of 8000 | elapsed:  1.5min remaining:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 8000 out of 8000 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "## NEAREST NEIGHBORS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "clf_name = 'knn'\n",
    "clf_options = {\n",
    "    'n_neighbors' : range(1, 6),\n",
    "    'weights' : ['uniform', 'distance'],\n",
    "    'algorithm' : ['auto'],\n",
    "    'leaf_size' : [10, 20, 30, 40, 50],\n",
    "    'p' : [1, 2, 3, 4],\n",
    "    'metric' : ['euclidean', 'manhattan', 'chebyshev', 'minkowski'],\n",
    "#     'metric_params' : [],\n",
    "#     'n_jobs' : []\n",
    "}\n",
    "\n",
    "# ## RANDOM FOREST\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier()\n",
    "# clf_name = 'rf'\n",
    "# clf_options = {\n",
    "#     'n_estimators' : range(100, 2001, 100),\n",
    "#     # 'criterion' : [],\n",
    "#     'max_features' : ['auto', 3, 4, 5],\n",
    "#     'max_depth' : [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "#     'min_samples_split' : [2, 5, 10],\n",
    "#     'min_samples_leaf' : [1, 2, 4],\n",
    "#     # 'min_weight_fraction_leaf' : [],\n",
    "#     # 'max_leaf_nodes' : [],\n",
    "#     # 'min_impurity_split' : [],\n",
    "#     # 'min_impurity_decrease' : [],\n",
    "#     'bootstrap' : [True, False]\n",
    "#     # 'oob_score' : [],\n",
    "# }\n",
    "\n",
    "\n",
    "# ## SVM\n",
    "# from sklearn.svm import SVC\n",
    "# clf = SVC()\n",
    "# clf_name = 'svm'\n",
    "# clf_options = {\n",
    "#     'C' : scipy.stats.expon(scale=100),\n",
    "#     'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#     'degree' : [2, 3, 4, 5],\n",
    "#     'gamma' : scipy.stats.expon(scale=.1),\n",
    "#     'coef0' : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#     'probability' : [True, False],\n",
    "#     'shrinking' : [True, False],\n",
    "#     'tol' : [1e-3, 1e-4],\n",
    "# #     'cache_size' : [],\n",
    "#     'class_weight' : [None, 'balanced'],\n",
    "# #     'verbose' : [],\n",
    "# #     'max_iter' : [],\n",
    "# #     'decision_function_shape' : []\n",
    "# #     'random_state' : []\n",
    "# }\n",
    "\n",
    "### DECISION TREE\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# clf = DecisionTreeClassifier()\n",
    "# clf_name = 'dt'\n",
    "# clf_options = {\n",
    "#     'criterion' : ['gini', 'entropy'],\n",
    "#     'splitter' : ['best', 'random'],\n",
    "#     'max_depth' : [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "#     'min_samples_split' : [2, 5, 10],\n",
    "#     'min_samples_leaf' : [1, 2, 4],\n",
    "# #     'min_weight_fraction_leaf' : [],\n",
    "#     'max_features' : ['auto', 'log2', 3, 4, 5, None],\n",
    "# #     'random_state' : [],\n",
    "# #     'max_leaf_nodes' : [],\n",
    "# #     'min_impurity_decrease' : [],\n",
    "# #     'min_impurity_split' : [],\n",
    "#     'class_weight' : [None, 'balanced'],\n",
    "#     'presort' : [True, False]\n",
    "# }\n",
    "\n",
    "\n",
    "best = getBestParams(clf, clf_options, search='grid', cv=10)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-282753cd23c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "best.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing knn through 25 trials\n",
      "trial 1 / 25 finished\n",
      "\r",
      "trial 2 / 25 finished\n",
      "\r",
      "trial 3 / 25 finished\n",
      "\r",
      "trial 4 / 25 finished\n",
      "\r",
      "trial 5 / 25 finished\n",
      "\r",
      "trial 6 / 25 finished\n",
      "\r",
      "trial 7 / 25 finished\n",
      "\r",
      "trial 8 / 25 finished\n",
      "\r",
      "trial 9 / 25 finished\n",
      "\r",
      "trial 10 / 25 finished\n",
      "\r",
      "trial 11 / 25 finished\n",
      "\r",
      "trial 12 / 25 finished\n",
      "\r",
      "trial 13 / 25 finished\n",
      "\r",
      "trial 14 / 25 finished\n",
      "\r",
      "trial 15 / 25 finished\n",
      "\r",
      "trial 16 / 25 finished\n",
      "\r",
      "trial 17 / 25 finished\n",
      "\r",
      "trial 18 / 25 finished\n",
      "\r",
      "trial 19 / 25 finished\n",
      "\r",
      "trial 20 / 25 finished\n",
      "\r",
      "trial 21 / 25 finished\n",
      "\r",
      "trial 22 / 25 finished\n",
      "\r",
      "trial 23 / 25 finished\n",
      "\r",
      "trial 24 / 25 finished\n",
      "\r",
      "trial 25 / 25 finished\n",
      "\r",
      "done\n"
     ]
    }
   ],
   "source": [
    "best_clf = KNeighborsClassifier(**best)\n",
    "record_results(best_clf, clf_name, trials=25)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
