{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune\n",
    "=====\n",
    "***\n",
    "\n",
    "## Importing modules\n",
    "This notebook trains and tests a model and dataset of your choice with various parameters using scikit-learn's `RandomizedSearchCV` and `GridSearchCV` functions to attempt to optimize the hyperparameters of the model. It stores the results into a separate file.\n",
    "\n",
    "The first step is to import the modules needed for calculation and data processing.\n",
    "* `numpy` is necessary for loading the dataset chosen\n",
    "* `sklearn.model_selection.GridSearchCV` and `RandomizedSearchCV` are the functions from scikit-learn that test a classifier with various parameters and returns the set of best parameters.\n",
    "* `time` is used for timing the train and test time for a classifier\n",
    "* `sklearn.metrics.accuracy_score`, `precision_score`, and `f1_score` are used to evalutate how well the optimized classifier performs\n",
    "* `scipy` is used to generate some ranges for the parameters in some classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide which dataset to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapoints: 33793\n",
      "num true datapoints: 16234\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs.full.TrainTest as TrainTest\n",
    "\n",
    "# p = 29\n",
    "# a = [2]\n",
    "w = 4\n",
    "cw = 15\n",
    "\n",
    "# path = './data/created_UniMiB-SHAR/nperseg=sqrt/'\n",
    "# file = 'p' + str(p) + '_a' + str(a) + '_w' + str(w) + '_cw' + str(cw)\n",
    "\n",
    "path = './data/created_collected/'\n",
    "file = 'w' + str(w) + '_cw' + str(cw)\n",
    "ext = '.npy'\n",
    "\n",
    "data = np.load(path + file + ext)\n",
    "\n",
    "print('datapoints: {}'.format(len(data)))\n",
    "true_count = 0\n",
    "for d in data:\n",
    "    if d[1]:\n",
    "        true_count += 1\n",
    "print('num true datapoints: {}'.format(true_count))\n",
    "\n",
    "x_train, y_train, x_test, y_test = TrainTest.get_train_test(data, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for getting the best parameters for a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParams(clf, param_dict, search='random', n_iter=100, cv=3):\n",
    "    if search == 'random':\n",
    "        clf_search = RandomizedSearchCV(\n",
    "            estimator = clf, \n",
    "            param_distributions = param_dict, \n",
    "            n_iter = n_iter, \n",
    "            cv = cv,\n",
    "            verbose = 1,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "    elif search == 'grid':\n",
    "        clf_search = GridSearchCV(\n",
    "            estimator = clf,\n",
    "            param_grid = param_dict,\n",
    "            cv = cv,\n",
    "            verbose = 1,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "    clf_search.fit(x_train, y_train)\n",
    "    return clf_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to record the results of classifier with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_results(clf, clf_name, trials=10):\n",
    "    print('testing {} through {} trials'.format(clf_name, trials))\n",
    "    avg_acc = 0\n",
    "    avg_prec = 0\n",
    "    avg_f1 = 0\n",
    "    avg_train_time = 0\n",
    "    avg_test_time = 0\n",
    "    \n",
    "    for i in range(trials):\n",
    "        train_start = time.clock()\n",
    "        clf.fit(x_train, y_train)\n",
    "        train_end = time.clock()\n",
    "        \n",
    "        test_start = time.clock()\n",
    "        y_pred = clf.predict(x_test)\n",
    "        test_end = time.clock()\n",
    "        \n",
    "        avg_acc += accuracy_score(y_test, y_pred)\n",
    "        avg_prec += precision_score(y_test, y_pred)\n",
    "        avg_f1 += f1_score(y_test, y_pred)\n",
    "        avg_train_time += (train_end - train_start)\n",
    "        avg_test_time += (test_end - test_start)\n",
    "        print('trial {} / {} finished\\n'.format(i + 1, trials), end='\\r')\n",
    "    \n",
    "    avg_acc /= trials\n",
    "    avg_prec /= trials\n",
    "    avg_f1 /= trials\n",
    "    avg_train_time /= trials\n",
    "    avg_test_time /= trials\n",
    "    \n",
    "    with open('./results/collected/' + clf_name + '/' + file + '.txt', 'a+') as f:\n",
    "        f.write('(best = {})\\n'.format(best))\n",
    "        f.write('trials         : {}\\n'.format(trials))\n",
    "        f.write('avg acc        : {}\\n'.format(avg_acc))\n",
    "        f.write('avg prec       : {}\\n'.format(avg_prec))\n",
    "        f.write('avg f1         : {}\\n'.format(avg_f1))\n",
    "        f.write('avg_train_time : {}\\n'.format(avg_train_time))\n",
    "        f.write('avg_test_time  : {}\\n'.format(avg_test_time))\n",
    "        f.write('-----\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define classifier and parameters to search through, then call functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  1.6min remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# ## NEAREST NEIGHBORS\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# clf = KNeighborsClassifier()\n",
    "# clf_name = 'knn'\n",
    "# clf_options = {\n",
    "#     'n_neighbors' : range(1, 6),\n",
    "#     'weights' : ['uniform', 'distance'],\n",
    "#     'algorithm' : ['auto'],\n",
    "#     'leaf_size' : [10, 20, 30, 40, 50],\n",
    "#     'p' : [1, 2, 3, 4],\n",
    "#     'metric' : ['euclidean', 'manhattan', 'chebyshev', 'minkowski'],\n",
    "# #     'metric_params' : [],\n",
    "# #     'n_jobs' : []\n",
    "# }\n",
    "\n",
    "# ## RANDOM FOREST\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier()\n",
    "# clf_name = 'rf'\n",
    "# clf_options = {\n",
    "#     'n_estimators' : range(100, 2001, 100),\n",
    "#     # 'criterion' : [],\n",
    "#     'max_features' : ['auto', 3, 4, 5],\n",
    "#     'max_depth' : [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "#     'min_samples_split' : [2, 5, 10],\n",
    "#     'min_samples_leaf' : [1, 2, 4],\n",
    "#     # 'min_weight_fraction_leaf' : [],\n",
    "#     # 'max_leaf_nodes' : [],\n",
    "#     # 'min_impurity_split' : [],\n",
    "#     # 'min_impurity_decrease' : [],\n",
    "#     'bootstrap' : [True, False]\n",
    "#     # 'oob_score' : [],\n",
    "# }\n",
    "\n",
    "\n",
    "## SVM\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf_name = 'svm'\n",
    "clf_options = {\n",
    "    'C' : scipy.stats.expon(scale=100),\n",
    "    'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree' : [2, 3, 4, 5],\n",
    "    'gamma' : scipy.stats.expon(scale=.1),\n",
    "    'coef0' : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'probability' : [True, False],\n",
    "    'shrinking' : [True, False],\n",
    "    'tol' : [1e-3, 1e-4],\n",
    "#     'cache_size' : [],\n",
    "    'class_weight' : [None, 'balanced'],\n",
    "#     'verbose' : [],\n",
    "#     'max_iter' : [],\n",
    "#     'decision_function_shape' : []\n",
    "#     'random_state' : []\n",
    "}\n",
    "\n",
    "### DECISION TREE\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# clf = DecisionTreeClassifier()\n",
    "# clf_name = 'dt'\n",
    "# clf_options = {\n",
    "#     'criterion' : ['gini', 'entropy'],\n",
    "#     'splitter' : ['best', 'random'],\n",
    "#     'max_depth' : [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "#     'min_samples_split' : [2, 5, 10],\n",
    "#     'min_samples_leaf' : [1, 2, 4],\n",
    "# #     'min_weight_fraction_leaf' : [],\n",
    "#     'max_features' : ['auto', 'log2', 3, 4, 5, None],\n",
    "# #     'random_state' : [],\n",
    "# #     'max_leaf_nodes' : [],\n",
    "# #     'min_impurity_decrease' : [],\n",
    "# #     'min_impurity_split' : [],\n",
    "#     'class_weight' : [None, 'balanced'],\n",
    "#     'presort' : [True, False]\n",
    "# }\n",
    "\n",
    "\n",
    "best = getBestParams(clf, clf_options, search='random', cv=2, n_iter=5)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-282753cd23c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "best.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing svm through 1 trials\n",
      "trial 1 / 1 finished\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "best_clf = SVC(**best)\n",
    "record_results(best_clf, clf_name, trials=1)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
